# Azure Data Engineering Overview

The present document gives an overview of data engineering tools in Microsoft AZURE.  

### Storage Accounts:  

It is simply a service that makes it possible to store data in the cloud in different ways:  
- Blob Storage ==> Blob Service
- Structured NoSQL data ==> Table service
- File Shares ==> File service
- Messages ==> Queue service (Messages sent by different applications)
  
We can activate the **"Eneble hierarchical namespace"** to create the **ADLS Gen2** datalake storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/363bb848-8d23-4d2f-8d3a-b77ee5611d23)  

It provides a unique namespace for our Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in our storage account is durable and highly available, secure, and massively scalable.  

**AVRO file format:**  
Row based storage.**Best suited for row filtering**  
it is a format that contains a json describing the data (metadata) and the data itself stored in binary format for better compressing performance and speed in terms of data transfer and storage.  

**Parquet file format:**  
Column based storage.**Best suited for column filtering**  

- Row-based storage: In row-based storage, data for a single row of a table is stored together in one block or page on disk. This means that all the columns of a given row are stored together, which can make it efficient for operations that need to retrieve an entire row of data at once, such as SELECT queries. However, row-based storage can be less efficient for operations that only need to access a subset of the columns in a table.
- Column-based storage: In column-based storage, each column of a table is stored separately on disk, which means that all the values for a given column are stored together. This can be more efficient for operations that only need to access a subset of the columns in a table, as the database can avoid reading in unnecessary data. Column-based storage can also be more efficient for certain types of queries, such as those that involve aggregations or calculations that only involve one column.
- Hybrid storage: Some databases use a hybrid storage approach that combines both row-based and column-based storage. In this approach, the database may store frequently accessed columns in a column-based format and less frequently accessed columns in a row-based format. This can provide the benefits of both approaches, but can also be more complex to implement and manage.
- Compressed (or Block Compressed) storage: Databases can also use compression techniques to reduce the amount of data that needs to be written to disk. For example, the database may use a compression algorithm to reduce the size of data before writing it to disk, and then decompress it when reading it back into memory. This can help to save disk space and reduce I/O operations, but can also add some overhead to the database's processing.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9a7664c6-c748-4e46-9750-9808f698e57e)  

**We can use ADF to convert CSV files to parquet files to optimize the storage**  

#### Create a shared access in ADLS:

We can access files in ADLS (or any account storage) by creating a shared access signature key:  

A shared access signature is a token that is appended to the URI for an Azure Storage resource. The token that contains a special set of query parameters that indicate how the resources may be accessed by the client.  

It provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:

- What resources the client may access.

- What permissions they have to those resources.

- How long the SAS is valid.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9fca2685-1079-48e0-87db-153a23a2e627)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ca629edd-4074-4aef-96b9-13f2dd5904bc)  

This SAS can be used to give access to Power BI for example.  

#### Keys:

We have mainly 3 types of keys in a database:  

- Primary key : defining unique rows in a table.
- Foreign key (primary key of another table): links two tables together to retrieve values from table 2 in table 1.
- Surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table.
- Alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID

We need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system.  

### Synaps : Design and implement data storage:

Synapse is simply a cloud data warehousing solution to create datawarehouses with an enhanced analytics capabilities.  

The flow can be presented as follows:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3b20a470-b0f6-4387-becf-8534363d1fbb)  

In a more detailed presentation:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/fa9dd3e5-aa08-48ac-a15a-c66bf660485e)  

As shown above, a synapse project belongs to a Synapse Workspace which belongs to a ressource group that is a part of an azure subscription.  

We start by creating a workspace:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3618a19a-6721-4916-b2ea-4f98ade7e91e)  

In the security step, we set the access method we desire to access the Storage of our data warehouse (SQL Pools Dedicated or serverless for example).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e6ba7091-32be-4671-9f99-87f3f809d828)  

#### Slowly Changing Dimensions (Dimension tables in DW):  

Slowly Changing Dimensions change over time, but at a slow pace and unpredictably. For example, a customer’s address in a retail business. When a customer moves, their address changes. If you overwrite the old address with the new one, you lose the history. But if you want to analyze historical sales data, you might need to know where the customer lived at the time of each sale. This is where SCDs come into play.  

There are several types of slowly changing dimensions in a data warehouse, with type 1 and type 2 being the most frequently used:  

- Type 0 SCD: The dimension attributes never change.
- Type 1 SCD: Overwrites existing data, doesn't keep history.
- Type 2 SCD: Adds new records for changes, keeps full history for a given natural key.
- Type 3 SCD: History is added as a new column.
- Type 4 SCD: A new dimension is added.
- Type 5 SCD: When certain attributes of a large dimension change over time, but using type 2 isn't feasible due to the dimension’s large size.
- Type 6 SCD: Combination of type 2 and type 3.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/56138ff9-78c9-40ea-b501-127cbea1c89a)  

The following example shows how to handle changes in a type 2 SCD for the Dim_Products table using T-SQL.
```sql
                      -- T-SQL
                      
                      IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')
                      BEGIN
                          -- Existing product record
                          UPDATE Dim_Products
                          SET ValidTo = GETDATE(), IsActive = 'False'
                          WHERE SourceKey = @ProductID AND IsActive = 'True';
                      END
                      ELSE
                      BEGIN
                          -- New product record
                          INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)
                          VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');
                      END
```
The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. Change Data Capture (CDC), change tracking, and triggers are all features available for managing data tracking in source systems such as SQL Server.  

#### Computing Options in synapse:

Synapse has two main computing options to perform analysis and store data:

- SQL serverless pools: Used to perform quick analysis of data (raw data in our data lake for example) to decide how to transform it and store it later on in the dedicated SQL pool. Only charged depending on the amount of data used.  
- SQL dedicated Pool: formerly called SQL DW, is the actual data warehouse where we store our transformed data in a permanent way to use in analytics later. The charge here is per DWU (data warehouse units).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ea879b3f-a2a3-4634-a1c9-98c696812172)  

#### 1. Serverless Sql Pool:  

In a first time setup of Serverless SQL pools we need to create a database to hold the views and external tables. Also we need to create credentials to be used by the DB to access the data sources.  

```sql
                          -- create master key that will protect the credentials:
                          CREATE MASTER KEY ENCRYPTION BY PASSWORD = <enter very strong password here>
                          
                          -- create credentials for containers in our demo storage account
                          CREATE DATABASE SCOPED CREDENTIAL sqlondemand
                          WITH IDENTITY='SHARED ACCESS SIGNATURE',  
                          SECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'
                          GO
                          CREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (
                              LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',
                              CREDENTIAL = sqlondemand
                          );
```

##### Connect to data sources in Synapse:

To connect to a ADLS Gen 2 for example, in the azure workspace we open azure synapse studio and we clcik the + button to add a data source.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/0d287e61-4588-4a85-b1d9-630e576173cc)  

This will ask us to create a linked service. **Note that the process is different for the other sources such SQL DB, KQL DB and Lakhouse.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/5951993e-831a-45f8-befb-e8ee7e9a7ffb)  

Before proceeding to querying data, we need to give access to the ADLS Gen 2 (data lake) where we have our blob storage.  

There are several ways to give access to storage accounts in general:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/2a76927a-4d64-4fb0-a21e-47ff222b1f00)  

https://devblogs.microsoft.com/devops/demystifying-service-principals-managed-identities/  

###### Quick clarification about the difference between Service Principals and managed identities:

Managed Identities are in essence 100% identical in functionality and use case than Service Principals. In fact, they are actually Service Principals.  

What makes them different though, is: 
– They are always linked to an Azure Resource, not to an application or 3rd party connector 
– They are automatically created for you, including the credentials; big benefit here is that no one knows the credentials.  

Managed Identities exist in 2 formats: 
– System assigned; in this scenario, the identity is linked to a single Azure Resource, eg a Virtual Machine, a Logic App, a Storage Account, Web App, Function,… so almost anything. Next, they also “live” with the Azure Resource, which means they get deleted when the Azure Resource gets deleted. 
– User Assigned Managed Identity, which means that you first have to create it as a stand-alone Azure resource by itself, after which it can be linked to multiple Azure Resources. An example here could be out of an integration with Key Vault, where different Workload services belonging to the same application stack, need to read out information from Key Vault. In this case, one could create a “read KV” Managed Identity, and link it to the web app, storage account, function, logic app,… all belonging to the same application architecture.  

Also, accessing an azure account storage can be done using account keys or shared access signature. The key difference is that :  
Access keys give us full rights to everything in our storage account, but with SAS we're able to limit the access capabilities of its users. We can limit capabilities such as read, write or update or to containers, plus we can timebox when the signature is valid for.  

Now we give access to our storage account:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/4593e9a2-b21b-40a4-861b-415b20b801c0)  

in the data lake ressource and under the IAM section we add the **Storage Blob Data Reader.** Then we select members to add (if using User or group option, otherwise we create a managed identity).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/1a8d873c-a2c2-42d9-822f-152eef878e9d)  

Then we can query data in our files (such as CSV files) using SQL queries with the **OPENROWSET** parsing function.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8c936e4d-4c67-4067-8f0a-a7f323f954e4)  

##### External Tables:  

External tables, are physical tables used to read or write data in Hadoop, Azure Blobs and ADLS.  
The can be created in both serverless pools and dedicated ones.  

The logic is that data is not loaded into the table, but remains in the storage and table reads directly from it using a **File Format**.  

2 main elements are needed before create an external table:  
- Creating an external data source where the data is stored as files.
- Creating an external file format (specifying the type of file, separator ..)

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/29082bf3-c535-4fb7-94a5-8a57efdeeab8)  

Creating the external data needs a database to hold it. Once the DB created we can created the external data source to point to the ALDS Gen2 for example. But this will require credentials to be defined to give access (Many types are possible, managed identity .. but we use here the SAS method, Shared Access Signature) to our database.  
For that, we create what we call **DATABASE SCOPED CREDENTIAL**, meaning creadentials that the database will use to access the storage account.  

```
                        CREATE DATABASE SCOPED CREDENTIAL credential_name
                        WITH IDENTITY = 'identity_name'
                        , SECRET = 'secret'
```

**But to prevent accessing to the secret in our DB, we should create a master key encryption to encrypte the secret and protect the credentials. The query will not run without it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/95dbe7fe-7f6d-4808-85da-9cc81fce4e65)  

**IDENTITY ='identity_name'**  
Specifies the name of the account to be used when connecting outside the server.  
To import a file from Azure Blob Storage or Azure Data Lake Storage using a shared key, the identity name must be SHARED ACCESS SIGNATURE. For more information about shared access signatures, see Using Shared Access Signatures (SAS). Only use IDENTITY = SHARED ACCESS SIGNATURE for a shared access signature.  
To import a file from Azure Blob Storage using a managed identity, the identity name must be MANAGED IDENTITY.  
When using Kerberos (Windows Active Directory or MIT KDC) do not use the domain name in the IDENTITY argument. It should just be the account name.  
In a SQL Server instance, if creating a database scoped credential with a Storage Access Key used as the SECRET, IDENTITY is ignored.  
WITH IDENTITY is not required if the container in Azure Blob storage is enabled for anonymous access.  

https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated  

Now wea can create our file format:  

```sql
                CREATE EXTERNAL FILE FORMAT TextFileFormat WITH (
                        FORRMAT_TYPE = DELIMITEDTEXT,
                        FORMAT_OPTIONS(
                              FIELD_TERMINATOR=',',
                              FIRST_ROW = 2
                              )
                )
```

Now we get to specify details of columns with their types with the **location**, **Data Source** and the **file format** name we created already:  
**If the headers contains space we can encapsulate them between brackets. They should be the same as the columns we specify in the CREATE query**

```sql
              CREATE EXTERNAL TABLE { database_name.schema_name.table_name | schema_name.table_name | table_name }
                  ( <column_definition> [ ,...n ] )
                  WITH (
                      LOCATION = 'hdfs_folder_or_filepath',
                      DATA_SOURCE = external_data_source_name,
                      FILE_FORMAT = external_file_format_name
                      [ , <reject_options> [ ,...n ] ]
                  )
```
**The location is either a folder to read all files at once or the exact file so that the create table reads from it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7769bd45-d1ef-4964-b73a-d713c94f6731)  

##### External Tables for parquet files:

When dealing with parquet files we need to specify the compression type used so it decompresse it an render the data from the binary format:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/baded98c-42e6-4211-9e97-9840cc9f8dbf)  

```sql
--Create an external file format for PARQUET files.
CREATE EXTERNAL FILE FORMAT file_format_name
WITH (
         FORMAT_TYPE = PARQUET
     [ , DATA_COMPRESSION = {
        'org.apache.hadoop.io.compress.SnappyCodec'
      | 'org.apache.hadoop.io.compress.GzipCodec' }
    ]);
```

**Note that in parquet files, headers are not allowed to have spaces. Also check the columns types in the parquet files given in the first part of the file.**  

##### OPENROWSET:  

It is a powerful function that allows you to access files in Azure Storage. OPENROWSET function reads content of a remote data source (for example file) and returns the content as a set of rows. Within the serverless SQL pool resource, the OPENROWSET bulk rowset provider is accessed by calling the OPENROWSET function and specifying the BULK option.  
The OPENROWSET function can be referenced in the FROM clause of a query as if it were a table name OPENROWSET. It supports bulk operations through a built-in BULK provider that enables data from a file to be read and returned as a rowset.  

**Authentication :** By default, OPENROWSET uses the Azure Active Directory to access the files in an ADLS Gen2 for example. So if we have access as users to the ADLS we can use OPENROWSET without credentials.  

#### Dedicated SQL pool:  

the dedicated SQL pools are needed for creating a real data warehouse (very structured data) to serve reporting needs. Note that the pricing is per hour of data processed and it is much more expensive then the serverless one.In the analytical pools section in synapse workspace we can create our dedicated pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/c145d21c-5259-42e9-9220-887aedf25833)  

Once clicking on create a new SQL pool (the plus button), we can set the caracteristics of our pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3beeec35-e83e-473e-820a-6572a486ee96)  

In opposition to serverless pools (where we can create only views and external tables), we can see that in dedicated pool we have a real database with tables views, external tables etc.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7064db39-3b2c-41db-9018-694e8c6a9b27)  

**Note that till now SQL dedicated pools don't support having multiple DBs in a single sql pool. Each pool have one single DB. (this is going to change and there will be a possibility to do crosse DB queries.**  

##### External tables in Dedicated pools:

For parquet files, Dedicated pools support natively the access to files. However for other formats it is not the case.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/26be217f-6d37-4f94-b3e6-c9af56f6aedd)  

For csv for example (and other formats), we need to create a hadoop external table to use it in order to access data.  

The authentication mode in SAS is not supported here so we should use access keys.  

Type| Dedicated Pools | Serverless Pools| 
| --- | --- | --- |
Storage authentication	| Storage Access Key(SAK), Microsoft Entra passthrough, Managed identity, custom application Microsoft Entra identity	| Shared Access Signature(SAS), Microsoft Entra passthrough, Managed identity, Custom application Microsoft Entra identity.|  

another main difference is that Folder partition elimination and File elimination (predicate pushdown) are not available in hadoop external tables.  

**Folder partition elimination**  
The native external tables in Synapse pools are able to ignore the files placed in the folders that are not relevant for the queries. If your files are stored in a folder hierarchy (for example - /year=2020/month=03/day=16) and the values for year, month, and day are exposed as the columns, the queries that contain filters like year=2020 will read the files only from the subfolders placed within the year=2020 folder. The files and folders placed in other folders (year=2021 or year=2022) will be ignored in this query. This elimination is known as partition elimination.  
The folder partition elimination is available in the native external tables that are synchronized from the Synapse Spark pools. If you have partitioned data set and you would like to leverage the partition elimination with the external tables that you create, use the partitioned views instead of the external tables.

**File elimination**  
Some data formats such as Parquet and Delta contain file statistics for each column (for example, min/max values for each column). The queries that filter data will not read the files where the required column values do not exist. The query will first explore min/max values for the columns used in the query predicate to find the files that do not contain the required data. These files will be ignored and eliminated from the query plan. This technique is also known as filter predicate pushdown and it can improve the performance of your queries. Filter pushdown is available in the serverless SQL pools on Parquet and Delta formats. To leverage filter pushdown for the string types, use the VARCHAR type with the Latin1_General_100_BIN2_UTF8 collation. For more information on collations, refer to Collation types supported for Synapse SQL.  

more in : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop  

The external data source for external tables (other than parquet) in dedicated pools should be changed as follows:  

```SQL
                  CREATE EXTERNAL DATA SOURCE AzureDataLakeStore
                  WITH
                    -- Please note the abfss endpoint when your account has secure transfer enabled
                    ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' , -- no need to add /blob name
                      CREDENTIAL = ADLS_credential ,
                      TYPE = HADOOP
                    ) ;
```

the lacation is now using **hdfs** protocol and **dfs endpoint** instead of blob to access the storage URL. Also note that we specify the name of the container before the **@**.  

Note that since we are forced to use hadoop external tables, the endpoint should be dfs. the difference between dfs and blob is the REST API used to access the storage:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8dabc61c-8f99-49c8-914e-c9ea873e3ecc)  

**So hadoop external tables needs ADLS Gen2 (Hierarchical namespace activated) to connect to data.**  

|External Data Source	| endpoint |
| --- | --- |
|Azure Blob Storage	| .blob |
|ADLS Gen 2	| .dfs |  

Example of creating an external table in Dedicated SQL Pool:  

```SQL
                            CREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential
                            WITH
                              IDENTITY = 'datalake2000233',
                              SECRET = 'ZuTNLYAYPx1gmjZHE2+pIizxi4zwOHBYX395HDVQ/wM47yfhHBEBppzMKVegfROIi99SCwCWD04F+AStQjvUxw==';
                            
                            
                            CREATE EXTERNAL DATA SOURCE log_data
                            WITH (    LOCATION   = 'abfss://csv@datalake2000233.dfs.core.windows.net', -- before the @ we specify the blob container name
                                      CREDENTIAL = AzureStorageCredential,
                                      TYPE = HADOOP
                            )
                            
                            
                            CREATE EXTERNAL FILE FORMAT TextFileFormat WITH (  
                                  FORMAT_TYPE = DELIMITEDTEXT,  
                                FORMAT_OPTIONS (  
                                    FIELD_TERMINATOR = ',',
                                    FIRST_ROW = 2))
                            
                            
                            
                            CREATE EXTERNAL TABLE [logdata]
                            (
                                [Correlation id] [varchar](200) NULL,
                            	[Operation name] [varchar](200) NULL,
                            	[Status] [varchar](100) NULL,
                            	[Event category] [varchar](100) NULL,
                            	[Level] [varchar](100) NULL,
                            	[Time] [varchar] NULL, 
                            	[Subscription] [varchar](200) NULL,
                            	[Event initiated by] [varchar](1000) NULL,
                            	[Resource type] [varchar](1000) NULL,
                            	[Resource group] [varchar](1000) NULL,
                                [Resource] [varchar](2000) NULL
                            )
                            WITH (
                             LOCATION = '/Log.csv',
                                DATA_SOURCE = log_data,  
                                FILE_FORMAT = TextFileFormat
                            )

```

Note that if we use [datetime] for the directly for the [Time] column, it will throw an error since the hadoop driver does not make the conversion by default from files using external tables (it is only supported in the serverless SQL pool). We need to specify the date format in the file format properties:  

```SQL
                          CREATE EXTERNAL FILE FORMAT csvFile_ddMMyyyy_fr2
                          WITH (
                              FORMAT_TYPE = DELIMITEDTEXT,
                              FORMAT_OPTIONS (
                                FIELD_TERMINATOR = ',',
                                STRING_DELIMITER = '"',
                               DATE_FORMAT = 'dd/MM/yyyy',
                                FIRST_ROW = 2,
                                USE_TYPE_DEFAULT = TRUE,
                                ENCODING = 'UTF8' )
                          );
```

**To reduce cost, we need to pause the dedicated SQL pool when not used! we can also create budgets to be notified when a limit is reached.**  

##### Loading data into persistent tables:

Loading data into the persistent table is done using the **COPY INTO** query:  

```
                            COPY INTO [ schema. ] table_name
                            [ (Column_list) ]
                            FROM '<external_location>' [ , ...n ]
                            WITH
                             (
                             [ FILE_TYPE = { 'CSV' | 'PARQUET' | 'ORC' } ]
                             [ , FILE_FORMAT = EXTERNAL FILE FORMAT OBJECT ]
                             [ , CREDENTIAL = (AZURE CREDENTIAL) ]
                             [ , ERRORFILE = ' [ http(s)://storageaccount/container ] /errorfile_directory [ / ] ] '
                             [ , ERRORFILE_CREDENTIAL = (AZURE CREDENTIAL) ]
                             [ , MAXERRORS = max_errors ]
                             [ , COMPRESSION = { 'Gzip' | 'DefaultCodec' | 'Snappy' } ]
                             [ , FIELDQUOTE = 'string_delimiter' ]
                             [ , FIELDTERMINATOR =  'field_terminator' ]
                             [ , ROWTERMINATOR = 'row_terminator' ]
                             [ , FIRSTROW = first_row ]
                             [ , DATEFORMAT = 'date_format' ]
                             [ , ENCODING = { 'UTF8' | 'UTF16' } ]
                             [ , IDENTITY_INSERT = { 'ON' | 'OFF' } ]
                             [ , AUTO_CREATE_TABLE = { 'ON' | 'OFF' } ]
                            )
```

Example :

```
                          COPY INTO [pool_logdata] FROM 'https://datalake244434.blob.core.windows.net/csv/Log.csv'
                          WITH(
                              FILE_TYPE='CSV',
                              CREDENTIAL=(IDENTITY='Storage Account Key',SECRET='vDV2bSKSR44lbE6x05HtFz57DvlK3O2WNkb11te+H+GrBjeXCojnHjiTw3KdYBWXJRSAnOAZNdgB+AStAasz8w=='),
                              FIRSTROW=2)
```

**Loading data can be done also using Bulk Load wizard in synapse:**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ba7a41ec-4743-46fe-9658-8c9d50853d1f)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/846f4c31-989c-4b9b-8230-c33355dc3ceb)  

##### DMV's for external ressources in synapse:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/80f75702-f956-4468-a0ee-506a627b94ed)  

##### Loading data using PolyBase:  

Loading data from external storages like ADLS Gen2, PolyBase is the fastest way.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/c8a3d2f9-68f6-4c43-bae8-d9a566c30f6f)  

PolyBase is a distributed compute engine inside SQL to improve performance when querying distributed data (Big Data).  

To COPY data using that, we just create extenal ressources (source, file format and external tables) and then copy into the persistent table from the external table.  

PolyBase with T-SQL works well when your data is in Azure Blob storage or Azure Data Lake Store. It gives you the most control over the loading process, but also requires you to define external data objects. The other methods define these objects behind the scenes as you map source tables to destination tables. To orchestrate T-SQL loads, you can use Azure Data Factory, SSIS, or Azure functions.  

##### Load data using Synapse Pipelines:  





