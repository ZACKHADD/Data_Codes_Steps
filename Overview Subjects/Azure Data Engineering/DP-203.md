# Azure Data Engineering Overview

The present document gives an overview of data engineering tools in Microsoft AZURE.  

### Storage Accounts:  

It is simply a service that makes it possible to store data in the cloud in different ways:  
- Blob Storage ==> Blob Service
- Structured NoSQL data ==> Table service
- File Shares ==> File service
- Messages ==> Queue service (Messages sent by different applications)
  
We can activate the **"Eneble hierarchical namespace"** to create the **ADLS Gen2** datalake storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/363bb848-8d23-4d2f-8d3a-b77ee5611d23)  

It provides a unique namespace for our Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in our storage account is durable and highly available, secure, and massively scalable.  

**AVRO file format:**  
Row based storage.**Best suited for row filtering**  
it is a format that contains a json describing the data (metadata) and the data itself stored in binary format for better compressing performance and speed in terms of data transfer and storage.  

**Parquet file format:**  
Column based storage.**Best suited for column filtering**  

- Row-based storage: In row-based storage, data for a single row of a table is stored together in one block or page on disk. This means that all the columns of a given row are stored together, which can make it efficient for operations that need to retrieve an entire row of data at once, such as SELECT queries. However, row-based storage can be less efficient for operations that only need to access a subset of the columns in a table.
- Column-based storage: In column-based storage, each column of a table is stored separately on disk, which means that all the values for a given column are stored together. This can be more efficient for operations that only need to access a subset of the columns in a table, as the database can avoid reading in unnecessary data. Column-based storage can also be more efficient for certain types of queries, such as those that involve aggregations or calculations that only involve one column.
- Hybrid storage: Some databases use a hybrid storage approach that combines both row-based and column-based storage. In this approach, the database may store frequently accessed columns in a column-based format and less frequently accessed columns in a row-based format. This can provide the benefits of both approaches, but can also be more complex to implement and manage.
- Compressed (or Block Compressed) storage: Databases can also use compression techniques to reduce the amount of data that needs to be written to disk. For example, the database may use a compression algorithm to reduce the size of data before writing it to disk, and then decompress it when reading it back into memory. This can help to save disk space and reduce I/O operations, but can also add some overhead to the database's processing.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9a7664c6-c748-4e46-9750-9808f698e57e)  

**We can use ADF to convert CSV files to parquet files to optimize the storage**  

#### Create a shared access in ADLS:

We can access files in ADLS (or any account storage) by creating a shared access signature key:  

A shared access signature is a token that is appended to the URI for an Azure Storage resource. The token that contains a special set of query parameters that indicate how the resources may be accessed by the client.  

It provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:

- What resources the client may access.

- What permissions they have to those resources.

- How long the SAS is valid.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9fca2685-1079-48e0-87db-153a23a2e627)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ca629edd-4074-4aef-96b9-13f2dd5904bc)  

This SAS can be used to give access to Power BI for example.  

#### Keys:

We have mainly 3 types of keys in a database:  

- Primary key : defining unique rows in a table.
- Foreign key (primary key of another table): links two tables together to retrieve values from table 2 in table 1.
- Surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table.
- Alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID

We need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system.  

### Synaps : Design and implement data storage:

Synapse is simply a cloud data warehousing solution to create datawarehouses with an enhanced analytics capabilities.  

The flow can be presented as follows:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3b20a470-b0f6-4387-becf-8534363d1fbb)  

In a more detailed presentation:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/fa9dd3e5-aa08-48ac-a15a-c66bf660485e)  

As shown above, a synapse project belongs to a Synapse Workspace which belongs to a ressource group that is a part of an azure subscription.  

We start by creating a workspace:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3618a19a-6721-4916-b2ea-4f98ade7e91e)  

In the security step, we set the access method we desire to access the Storage of our data warehouse (SQL Pools Dedicated or serverless for example).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e6ba7091-32be-4671-9f99-87f3f809d828)  

#### Slowly Changing Dimensions (Dimension tables in DW):  

Slowly Changing Dimensions change over time, but at a slow pace and unpredictably. For example, a customer’s address in a retail business. When a customer moves, their address changes. If you overwrite the old address with the new one, you lose the history. But if you want to analyze historical sales data, you might need to know where the customer lived at the time of each sale. This is where SCDs come into play.  

There are several types of slowly changing dimensions in a data warehouse, with type 1 and type 2 being the most frequently used:  

- Type 0 SCD: The dimension attributes never change.
- Type 1 SCD: Overwrites existing data, doesn't keep history.
- Type 2 SCD: Adds new records for changes, keeps full history for a given natural key.
- Type 3 SCD: History is added as a new column.
- Type 4 SCD: A new dimension is added.
- Type 5 SCD: When certain attributes of a large dimension change over time, but using type 2 isn't feasible due to the dimension’s large size.
- Type 6 SCD: Combination of type 2 and type 3.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/56138ff9-78c9-40ea-b501-127cbea1c89a)  

The following example shows how to handle changes in a type 2 SCD for the Dim_Products table using T-SQL.
```sql
                      -- T-SQL
                      
                      IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')
                      BEGIN
                          -- Existing product record
                          UPDATE Dim_Products
                          SET ValidTo = GETDATE(), IsActive = 'False'
                          WHERE SourceKey = @ProductID AND IsActive = 'True';
                      END
                      ELSE
                      BEGIN
                          -- New product record
                          INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)
                          VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');
                      END
```
The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. Change Data Capture (CDC), change tracking, and triggers are all features available for managing data tracking in source systems such as SQL Server.  

#### Computing Options in synapse:

Synapse has two main computing options to perform analysis and store data:

- SQL serverless pools: Used to perform quick analysis of data (raw data in our data lake for example) to decide how to transform it and store it later on in the dedicated SQL pool. Only charged depending on the amount of data used.  
- SQL dedicated Pool: formerly called SQL DW, is the actual data warehouse where we store our transformed data in a permanent way to use in analytics later. The charge here is per DWU (data warehouse units).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ea879b3f-a2a3-4634-a1c9-98c696812172)  

#### Connect to data sources in Synapse:

To connect to a ADLS Gen 2 for example, in the azure workspace we open azure synapse studio and we clcik the + button to add a data source.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/0d287e61-4588-4a85-b1d9-630e576173cc)  

This will ask us to create a linked service. **Note that the process is different for the other sources such SQL DB, KQL DB and Lakhouse.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/5951993e-831a-45f8-befb-e8ee7e9a7ffb)  

Before proceeding to querying data, we need to give access to the ADLS Gen 2 (data lake) where we have our blob storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/4593e9a2-b21b-40a4-861b-415b20b801c0)  

in the data lake ressource and under the IAM section we add the **Storage Blob Data Reader.** Then we select members to add (if using User or group option, otherwise we create a managed identity).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/1a8d873c-a2c2-42d9-822f-152eef878e9d)  

Then we can query data in our files (such as CSV files) using SQL queries with the **OPENROWSET** parsing function.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8c936e4d-4c67-4067-8f0a-a7f323f954e4)  

#### External Tables:  

External tables, are physical tables used to read or write data in Hadoop, Azure Blobs and ADLS.  
The can be created in both serverless pools and dedicated ones.  

The logic is that data is not loaded into the table, but remains in the storage and table reads directly from it using a **File Format**.  

2 main elements are needed before create an external table:  
- Creating an external data source where the data is stored as files.
- Creating an external file format (specifying the type of file, separator ..)

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/29082bf3-c535-4fb7-94a5-8a57efdeeab8)  

Creating the external data needs a database to hold it. Once the DB created we can created the external data source to point to the ALDS Gen2 for example. But this will require credentials to be defined to give access (Many types are possible, managed identity .. but we use here the SAS method, Shared Access Signature) to our database.  
For that, we create what we call **DATABASE SCOPED CREDENTIAL**, meaning creadentials that the database will use to access the storage account.  

```
                        CREATE DATABASE SCOPED CREDENTIAL credential_name
                        WITH IDENTITY = 'identity_name'
                        , SECRET = 'secret'
```

**But to prevent accessing to the secret in our DB, we should create a master key encryption to encrypte the secret. The query will not run without it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/95dbe7fe-7f6d-4808-85da-9cc81fce4e65)  

**IDENTITY ='identity_name'**  
Specifies the name of the account to be used when connecting outside the server.  
To import a file from Azure Blob Storage or Azure Data Lake Storage using a shared key, the identity name must be SHARED ACCESS SIGNATURE. For more information about shared access signatures, see Using Shared Access Signatures (SAS). Only use IDENTITY = SHARED ACCESS SIGNATURE for a shared access signature.  
To import a file from Azure Blob Storage using a managed identity, the identity name must be MANAGED IDENTITY.  
When using Kerberos (Windows Active Directory or MIT KDC) do not use the domain name in the IDENTITY argument. It should just be the account name.  
In a SQL Server instance, if creating a database scoped credential with a Storage Access Key used as the SECRET, IDENTITY is ignored.  
WITH IDENTITY is not required if the container in Azure Blob storage is enabled for anonymous access.  

https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated  

Now wea can create our file format:  

```sql
                CREATE EXTERNAL FILE FORMAT TextFileFormat WITH (
                        FORRMAT_TYPE = DELIMITEDTEXT,
                        FORMAT_OPTIONS(
                              FIELD_TERMINATOR=',',
                              FIRST_ROW = 2
                              )
                )
```

Now we get to specify details of columns with their types with the **location**, **Data Source** and the **file format** name we created already:  
**If the headers contains space we can encapsulate them between brackets. They should be the same as the columns we specify in the CREATE query**

```sql
              CREATE EXTERNAL TABLE { database_name.schema_name.table_name | schema_name.table_name | table_name }
                  ( <column_definition> [ ,...n ] )
                  WITH (
                      LOCATION = 'hdfs_folder_or_filepath',
                      DATA_SOURCE = external_data_source_name,
                      FILE_FORMAT = external_file_format_name
                      [ , <reject_options> [ ,...n ] ]
                  )
```
**The location is either a folder to read all files at once or the exact file so that the create table reads from it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7769bd45-d1ef-4964-b73a-d713c94f6731)  

##### External Tables for parquet files:

When dealing with parquet files we need to specify the compression type used so it decompresse it an render the data from the binary format:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/baded98c-42e6-4211-9e97-9840cc9f8dbf)  

```sql
--Create an external file format for PARQUET files.
CREATE EXTERNAL FILE FORMAT file_format_name
WITH (
         FORMAT_TYPE = PARQUET
     [ , DATA_COMPRESSION = {
        'org.apache.hadoop.io.compress.SnappyCodec'
      | 'org.apache.hadoop.io.compress.GzipCodec' }
    ]);
```

**Note that in parquet files, headers are not allowed to have spaces. Also check the columns types in the parquet files given in the first part of the file.**  

##### OPENROWSET:  


