# Azure Data Engineering Overview

The present document gives an overview of data engineering tools in Microsoft AZURE.  

### Storage Accounts:  

It is simply a service that makes it possible to store data in the cloud in different ways:  
- Blob Storage ==> Blob Service
- Structured NoSQL data ==> Table service
- File Shares ==> File service
- Messages ==> Queue service (Messages sent by different applications)
  
We can activate the **"Eneble hierarchical namespace"** to create the **ADLS Gen2** datalake storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/363bb848-8d23-4d2f-8d3a-b77ee5611d23)  

It provides a unique namespace for our Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in our storage account is durable and highly available, secure, and massively scalable.  

**AVRO file format:**  
Row based storage.**Best suited for row filtering**  
it is a format that contains a json describing the data (metadata) and the data itself stored in binary format for better compressing performance and speed in terms of data transfer and storage.  

**Parquet file format:**  
Column based storage.**Best suited for column filtering**  

- Row-based storage: In row-based storage, data for a single row of a table is stored together in one block or page on disk. This means that all the columns of a given row are stored together, which can make it efficient for operations that need to retrieve an entire row of data at once, such as SELECT queries. However, row-based storage can be less efficient for operations that only need to access a subset of the columns in a table.
- Column-based storage: In column-based storage, each column of a table is stored separately on disk, which means that all the values for a given column are stored together. This can be more efficient for operations that only need to access a subset of the columns in a table, as the database can avoid reading in unnecessary data. Column-based storage can also be more efficient for certain types of queries, such as those that involve aggregations or calculations that only involve one column.
- Hybrid storage: Some databases use a hybrid storage approach that combines both row-based and column-based storage. In this approach, the database may store frequently accessed columns in a column-based format and less frequently accessed columns in a row-based format. This can provide the benefits of both approaches, but can also be more complex to implement and manage.
- Compressed (or Block Compressed) storage: Databases can also use compression techniques to reduce the amount of data that needs to be written to disk. For example, the database may use a compression algorithm to reduce the size of data before writing it to disk, and then decompress it when reading it back into memory. This can help to save disk space and reduce I/O operations, but can also add some overhead to the database's processing.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9a7664c6-c748-4e46-9750-9808f698e57e)  

**We can use ADF to convert CSV files to parquet files to optimize the storage**  

#### Create a shared access in ADLS:

We can access files in ADLS (or any account storage) by creating a shared access signature key:  

A shared access signature is a token that is appended to the URI for an Azure Storage resource. The token that contains a special set of query parameters that indicate how the resources may be accessed by the client.  

It provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:

- What resources the client may access.

- What permissions they have to those resources.

- How long the SAS is valid.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9fca2685-1079-48e0-87db-153a23a2e627)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ca629edd-4074-4aef-96b9-13f2dd5904bc)  

This SAS can be used to give access to Power BI for example.  

#### Keys:

We have mainly 3 types of keys in a database:  

- Primary key : defining unique rows in a table.
- Foreign key (primary key of another table): links two tables together to retrieve values from table 2 in table 1.
- Surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table.
- Alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID

We need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system.  

### Synaps : Design and implement data storage:

Synapse is simply a cloud data warehousing solution to create datawarehouses with an enhanced analytics capabilities.  

The flow can be presented as follows:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3b20a470-b0f6-4387-becf-8534363d1fbb)  

In a more detailed presentation:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/fa9dd3e5-aa08-48ac-a15a-c66bf660485e)  

As shown above, a synapse project belongs to a Synapse Workspace which belongs to a ressource group that is a part of an azure subscription.  

We start by creating a workspace:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3618a19a-6721-4916-b2ea-4f98ade7e91e)  

In the security step, we set the access method we desire to access the Storage of our data warehouse (SQL Pools Dedicated or serverless for example).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e6ba7091-32be-4671-9f99-87f3f809d828)  

#### Slowly Changing Dimensions (Dimension tables in DW):  

Slowly Changing Dimensions change over time, but at a slow pace and unpredictably. For example, a customer’s address in a retail business. When a customer moves, their address changes. If you overwrite the old address with the new one, you lose the history. But if you want to analyze historical sales data, you might need to know where the customer lived at the time of each sale. This is where SCDs come into play.  

There are several types of slowly changing dimensions in a data warehouse, with type 1 and type 2 being the most frequently used:  

- Type 0 SCD: The dimension attributes never change.
- Type 1 SCD: Overwrites existing data, doesn't keep history.
- Type 2 SCD: Adds new records for changes, keeps full history for a given natural key.
- Type 3 SCD: History is added as a new column.
- Type 4 SCD: A new dimension is added.
- Type 5 SCD: When certain attributes of a large dimension change over time, but using type 2 isn't feasible due to the dimension’s large size.
- Type 6 SCD: Combination of type 2 and type 3.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/56138ff9-78c9-40ea-b501-127cbea1c89a)  

The following example shows how to handle changes in a type 2 SCD for the Dim_Products table using T-SQL.
```
                      -- T-SQL
                      
                      IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')
                      BEGIN
                          -- Existing product record
                          UPDATE Dim_Products
                          SET ValidTo = GETDATE(), IsActive = 'False'
                          WHERE SourceKey = @ProductID AND IsActive = 'True';
                      END
                      ELSE
                      BEGIN
                          -- New product record
                          INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)
                          VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');
                      END
```
The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. Change Data Capture (CDC), change tracking, and triggers are all features available for managing data tracking in source systems such as SQL Server.  

#### Computing Options in synapse:

Synapse has two main computing options to perform analysis and store data:

- SQL serverless pools: Used to perform quick analysis of data (raw data in our data lake for example) to decide how to transform it and store it later on in the dedicated SQL pool. Only charged depending on the amount of data used.  
- SQL dedicated Pool: formerly called SQL DW, is the actual data warehouse where we store our transformed data in a permanent way to use in analytics later. The charge here is per DWU (data warehouse units).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ea879b3f-a2a3-4634-a1c9-98c696812172)  

#### Connect to data sources in Synapse:

To connect to a ADLS Gen 2 for example, in the azure workspace we open azure synapse studio and we clcik the + button to add a data source.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/0d287e61-4588-4a85-b1d9-630e576173cc)  

This will ask us to create a linked service. **Note that the process is different for the other sources such SQL DB, KQL DB and Lakhouse.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/5951993e-831a-45f8-befb-e8ee7e9a7ffb)  

Before proceeding to querying data, we need to give access to the ADLS Gen 2 (data lake) where we have our blob storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/4593e9a2-b21b-40a4-861b-415b20b801c0)  

in the data lake ressource and under the IAM section we add the **Storage Blob Data Reader.** Then we select members to add (if using User or group option, otherwise we create a managed identity).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/1a8d873c-a2c2-42d9-822f-152eef878e9d)  

Then we can query data in our files (such as CSV files) using SQL queries with the **OPENROWSET** parsing function.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8c936e4d-4c67-4067-8f0a-a7f323f954e4)  

