# Azure Data Engineering Overview

The present document gives an overview of data engineering tools in Microsoft AZURE.  

### Storage Accounts:  

It is simply a service that makes it possible to store data in the cloud in different ways:  
- Blob Storage ==> Blob Service
- Structured NoSQL data ==> Table service
- File Shares ==> File service
- Messages ==> Queue service (Messages sent by different applications)
  
We can activate the **"Eneble hierarchical namespace"** to create the **ADLS Gen2** datalake storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/363bb848-8d23-4d2f-8d3a-b77ee5611d23)  

It provides a unique namespace for our Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in our storage account is durable and highly available, secure, and massively scalable.  

**AVRO file format:**  
Row based storage.**Best suited for row filtering**  
it is a format that contains a json describing the data (metadata) and the data itself stored in binary format for better compressing performance and speed in terms of data transfer and storage.  

**Parquet file format:**  
Column based storage.**Best suited for column filtering**  

- Row-based storage: In row-based storage, data for a single row of a table is stored together in one block or page on disk. This means that all the columns of a given row are stored together, which can make it efficient for operations that need to retrieve an entire row of data at once, such as SELECT queries. However, row-based storage can be less efficient for operations that only need to access a subset of the columns in a table.
- Column-based storage: In column-based storage, each column of a table is stored separately on disk, which means that all the values for a given column are stored together. This can be more efficient for operations that only need to access a subset of the columns in a table, as the database can avoid reading in unnecessary data. Column-based storage can also be more efficient for certain types of queries, such as those that involve aggregations or calculations that only involve one column.
- Hybrid storage: Some databases use a hybrid storage approach that combines both row-based and column-based storage. In this approach, the database may store frequently accessed columns in a column-based format and less frequently accessed columns in a row-based format. This can provide the benefits of both approaches, but can also be more complex to implement and manage.
- Compressed (or Block Compressed) storage: Databases can also use compression techniques to reduce the amount of data that needs to be written to disk. For example, the database may use a compression algorithm to reduce the size of data before writing it to disk, and then decompress it when reading it back into memory. This can help to save disk space and reduce I/O operations, but can also add some overhead to the database's processing.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9a7664c6-c748-4e46-9750-9808f698e57e)  

**We can use ADF to convert CSV files to parquet files to optimize the storage**  

#### Create a shared access in ADLS:

We can access files in ADLS (or any account storage) by creating a shared access signature key:  

A shared access signature is a token that is appended to the URI for an Azure Storage resource. The token that contains a special set of query parameters that indicate how the resources may be accessed by the client.  

It provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:

- What resources the client may access.

- What permissions they have to those resources.

- How long the SAS is valid.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9fca2685-1079-48e0-87db-153a23a2e627)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ca629edd-4074-4aef-96b9-13f2dd5904bc)  

This SAS can be used to give access to Power BI for example.  

#### Keys:

We have mainly 3 types of keys in a database:  

- Primary key : defining unique rows in a table.
- Foreign key (primary key of another table): links two tables together to retrieve values from table 2 in table 1.
- Surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table.
- Alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID

We need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system.  

### Synaps : Design and implement data storage:

Synapse is simply a cloud data warehousing solution to create datawarehouses with an enhanced analytics capabilities.  

The flow can be presented as follows:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3b20a470-b0f6-4387-becf-8534363d1fbb)  

In a more detailed presentation:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/fa9dd3e5-aa08-48ac-a15a-c66bf660485e)  

As shown above, a synapse project belongs to a Synapse Workspace which belongs to a ressource group that is a part of an azure subscription.  

We start by creating a workspace:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3618a19a-6721-4916-b2ea-4f98ade7e91e)  

In the security step, we set the access method we desire to access the Storage of our data warehouse (SQL Pools Dedicated or serverless for example).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e6ba7091-32be-4671-9f99-87f3f809d828)  

#### Computing Options in synapse:

Synapse has two main computing options to perform analysis and store data:

- SQL serverless pools: Used to perform quick analysis of data (raw data in our data lake for example) to decide how to transform it and store it later on in the dedicated SQL pool.
- SQL dedicated Pool: formerly called SQL DW, is the actual data warehouse wher we store our transformed data to use in analytics later.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ea879b3f-a2a3-4634-a1c9-98c696812172)  


