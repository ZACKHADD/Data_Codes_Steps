# Azure Data Engineering Overview

The present document gives an overview of data engineering tools in Microsoft AZURE.  

Data engineering architecture example:  

![image](https://github.com/user-attachments/assets/b0795fc8-4b55-427d-bada-ea11c04b8842)

### Storage Accounts:  

It is simply a service that makes it possible to store data in the cloud in different ways:  
- Blob Storage ==> Blob Service
- Structured NoSQL data ==> Table service
- File Shares ==> File service
  
  ![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/bddc05b3-a2af-4adb-a270-f6562ad8bd4e)
  
It depends mostly on your use-case and how you plan to access the data. If you simply want to mount and access your files Azure Files will be your best fit. If you are looking for the lowest cost and want to access your data programmatically through your application Azure Blob would be a better fit.
- Messages ==> Queue service (Messages sent by different applications)
  
We can activate the **"Enable hierarchical namespace"** to create the **ADLS Gen2** datalake storage.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/363bb848-8d23-4d2f-8d3a-b77ee5611d23)  

It provides a unique namespace for our Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in our storage account is durable and highly available, secure, and massively scalable.  

**AVRO file format:**  
Row based storage.**Best suited for row filtering**  
it is a format that contains a json describing the data (metadata) and the data itself stored in binary format for better compressing performance and speed in terms of data transfer and storage.  

**Parquet file format:**  
Column based storage.**Best suited for column filtering**  

- Row-based storage: In row-based storage, data for a single row of a table is stored together in one block or page on disk. This means that all the columns of a given row are stored together, which can make it efficient for operations that need to retrieve an entire row of data at once, such as SELECT queries. However, row-based storage can be less efficient for operations that only need to access a subset of the columns in a table.
- Column-based storage: In column-based storage, each column of a table is stored separately on disk, which means that all the values for a given column are stored together. This can be more efficient for operations that only need to access a subset of the columns in a table, as the database can avoid reading in unnecessary data. Column-based storage can also be more efficient for certain types of queries, such as those that involve aggregations or calculations that only involve one column.
- Hybrid storage: Some databases use a hybrid storage approach that combines both row-based and column-based storage. In this approach, the database may store frequently accessed columns in a column-based format and less frequently accessed columns in a row-based format. This can provide the benefits of both approaches, but can also be more complex to implement and manage.
- Compressed (or Block Compressed) storage: Databases can also use compression techniques to reduce the amount of data that needs to be written to disk. For example, the database may use a compression algorithm to reduce the size of data before writing it to disk, and then decompress it when reading it back into memory. This can help to save disk space and reduce I/O operations, but can also add some overhead to the database's processing.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9a7664c6-c748-4e46-9750-9808f698e57e)  

**We can use ADF to convert CSV files to parquet files to optimize the storage**  

#### Create a shared access in ADLS:

We can access files in ADLS (or any account storage) by creating a shared access signature key:  

A shared access signature is a token that is appended to the URI for an Azure Storage resource. The token that contains a special set of query parameters that indicate how the resources may be accessed by the client.  

It provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:

- What resources the client may access.

- What permissions they have to those resources.

- How long the SAS is valid.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/9fca2685-1079-48e0-87db-153a23a2e627)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ca629edd-4074-4aef-96b9-13f2dd5904bc)  

This SAS can be used to give access to Power BI for example.  

#### Secure the access to the resources via Key Vaults:  

An important thing to keep in mind when dealing with credentials is to store them in key vaults and create secrets for each credential (Username and Password one secret for each one). This encrypts the credentials and prevents from exposing the credentials in the architecture of the project (not typing the credentials explicitely in th ressources).  
Bare in mind also if we work with on premise database we should create roles and give permissions to the role so that the cloud ressource Data Factory for instance can access the database. Also do not forget to install on the on premise server/machine the self hosted runtime (it is basically a gateway) to build the connectin between the cloud and the on premise machines.  

#### Keys:

We have mainly 3 types of keys in a database:  

- Primary key : defining unique rows in a table.
- Foreign key (primary key of another table): links two tables together to retrieve values from table 2 in table 1.
- Surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table.
- Alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID

We need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system.  

### Synaps : Design and implement data storage:

Synapse is simply a cloud data warehousing solution to create datawarehouses with an enhanced analytics capabilities.  

The flow can be presented as follows:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3b20a470-b0f6-4387-becf-8534363d1fbb)  

In a more detailed presentation:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/fa9dd3e5-aa08-48ac-a15a-c66bf660485e)  

As shown above, a synapse project belongs to a Synapse Workspace which belongs to a ressource group that is a part of an azure subscription.  

We start by creating a workspace:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3618a19a-6721-4916-b2ea-4f98ade7e91e)  

In the security step, we set the access method we desire to access the Storage of our data warehouse (SQL Pools Dedicated or serverless for example).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e6ba7091-32be-4671-9f99-87f3f809d828)  

#### Slowly Changing Dimensions (Dimension tables in DW):  

Slowly Changing Dimensions change over time, but at a slow pace and unpredictably. For example, a customer’s address in a retail business. When a customer moves, their address changes. If you overwrite the old address with the new one, you lose the history. But if you want to analyze historical sales data, you might need to know where the customer lived at the time of each sale. This is where SCDs come into play.  

There are several types of slowly changing dimensions in a data warehouse, with type 1 and type 2 being the most frequently used:  

- Type 0 SCD: The dimension attributes never change.
- Type 1 SCD: Overwrites existing data, doesn't keep history.
- Type 2 SCD: Adds new records for changes, keeps full history for a given natural key.
- Type 3 SCD: History is added as a new column.
- Type 4 SCD: A new dimension is added.
- Type 5 SCD: When certain attributes of a large dimension change over time, but using type 2 isn't feasible due to the dimension’s large size.
- Type 6 SCD: Combination of type 2 and type 3.

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/56138ff9-78c9-40ea-b501-127cbea1c89a)  

The following example shows how to handle changes in a type 2 SCD for the Dim_Products table using T-SQL.
```sql
                      -- T-SQL
                      
                      IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')
                      BEGIN
                          -- Existing product record
                          UPDATE Dim_Products
                          SET ValidTo = GETDATE(), IsActive = 'False'
                          WHERE SourceKey = @ProductID AND IsActive = 'True';
                      END
                      ELSE
                      BEGIN
                          -- New product record
                          INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)
                          VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');
                      END
```
The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. Change Data Capture (CDC), change tracking, and triggers are all features available for managing data tracking in source systems such as SQL Server.  

#### Computing Options in synapse:

Synapse has two main computing options to perform analysis and store data:

- SQL serverless pools: Used to perform quick analysis of data (raw data in our data lake for example) to decide how to transform it and store it later on in the dedicated SQL pool. Only charged depending on the amount of data used.  
- SQL dedicated Pool: formerly called SQL DW, is the actual data warehouse where we store our transformed data in a permanent way to use in analytics later. The charge here is per DWU (data warehouse units).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ea879b3f-a2a3-4634-a1c9-98c696812172)  

#### 1. Serverless Sql Pool:  

In a first time setup of Serverless SQL pools we need to create a database to hold the views and external tables. Also we need to create credentials to be used by the DB to access the data sources.  

```sql
                          -- create master key that will protect the credentials:
                          CREATE MASTER KEY ENCRYPTION BY PASSWORD = <enter very strong password here>
                          
                          -- create credentials for containers in our demo storage account
                          CREATE DATABASE SCOPED CREDENTIAL sqlondemand
                          WITH IDENTITY='SHARED ACCESS SIGNATURE',  
                          SECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'
                          GO
                          CREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (
                              LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',
                              CREDENTIAL = sqlondemand
                          );
```

##### Connect to data sources in Synapse:

To connect to a ADLS Gen 2 for example, in the azure workspace we open azure synapse studio and we clcik the + button to add a data source.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/0d287e61-4588-4a85-b1d9-630e576173cc)  

This will ask us to create a linked service. **Note that the process is different for the other sources such SQL DB, KQL DB and Lakhouse.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/5951993e-831a-45f8-befb-e8ee7e9a7ffb)  

Before proceeding to querying data, we need to give access to the ADLS Gen 2 (data lake) where we have our blob storage.  

There are several ways to give access to storage accounts in general:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/2a76927a-4d64-4fb0-a21e-47ff222b1f00)  

https://devblogs.microsoft.com/devops/demystifying-service-principals-managed-identities/  

###### Quick clarification about the difference between Service Principals and managed identities:

Managed Identities are in essence 100% identical in functionality and use case than Service Principals. In fact, they are actually Service Principals.  

What makes them different though, is: 
– They are always linked to an Azure Resource, not to an application or 3rd party connector 
– They are automatically created for you, including the credentials; big benefit here is that no one knows the credentials.  

Managed Identities exist in 2 formats: 
– System assigned; in this scenario, the identity is linked to a single Azure Resource, eg a Virtual Machine, a Logic App, a Storage Account, Web App, Function,… so almost anything. Next, they also “live” with the Azure Resource, which means they get deleted when the Azure Resource gets deleted. 
– User Assigned Managed Identity, which means that you first have to create it as a stand-alone Azure resource by itself, after which it can be linked to multiple Azure Resources. An example here could be out of an integration with Key Vault, where different Workload services belonging to the same application stack, need to read out information from Key Vault. In this case, one could create a “read KV” Managed Identity, and link it to the web app, storage account, function, logic app,… all belonging to the same application architecture.  

Also, accessing an azure account storage can be done using account keys or shared access signature. The key difference is that :  
Access keys give us full rights to everything in our storage account, but with SAS we're able to limit the access capabilities of its users. We can limit capabilities such as read, write or update or to containers, plus we can timebox when the signature is valid for.  

Now we give access to our storage account:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/4593e9a2-b21b-40a4-861b-415b20b801c0)  

in the data lake ressource and under the IAM section we add the **Storage Blob Data Reader.** Then we select members to add (if using User or group option, otherwise we create a managed identity).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/1a8d873c-a2c2-42d9-822f-152eef878e9d)  

Then we can query data in our files (such as CSV files) using SQL queries with the **OPENROWSET** parsing function.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8c936e4d-4c67-4067-8f0a-a7f323f954e4)  

##### External Tables:  

External tables, are physical tables used to read or write data in Hadoop, Azure Blobs and ADLS.  
The can be created in both serverless pools and dedicated ones.  

The logic is that data is not loaded into the table, but remains in the storage and table reads directly from it using a **File Format**.  

2 main elements are needed before create an external table:  
- Creating an external data source where the data is stored as files.
- Creating an external file format (specifying the type of file, separator ..)

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/29082bf3-c535-4fb7-94a5-8a57efdeeab8)  

Creating the external data needs a database to hold it. Once the DB created we can created the external data source to point to the ALDS Gen2 for example. But this will require credentials to be defined to give access (Many types are possible, managed identity .. but we use here the SAS method, Shared Access Signature) to our database.  
For that, we create what we call **DATABASE SCOPED CREDENTIAL**, meaning creadentials that the database will use to access the storage account.  

```
                        CREATE DATABASE SCOPED CREDENTIAL credential_name
                        WITH IDENTITY = 'identity_name'
                        , SECRET = 'secret'
```

**But to prevent accessing to the secret in our DB, we should create a master key encryption to encrypte the secret and protect the credentials. The query will not run without it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/95dbe7fe-7f6d-4808-85da-9cc81fce4e65)  

**IDENTITY ='identity_name'**  
Specifies the name of the account to be used when connecting outside the server.  
To import a file from Azure Blob Storage or Azure Data Lake Storage using a shared key, the identity name must be SHARED ACCESS SIGNATURE. For more information about shared access signatures, see Using Shared Access Signatures (SAS). Only use IDENTITY = SHARED ACCESS SIGNATURE for a shared access signature.  
To import a file from Azure Blob Storage using a managed identity, the identity name must be MANAGED IDENTITY.  
When using Kerberos (Windows Active Directory or MIT KDC) do not use the domain name in the IDENTITY argument. It should just be the account name.  
In a SQL Server instance, if creating a database scoped credential with a Storage Access Key used as the SECRET, IDENTITY is ignored.  
WITH IDENTITY is not required if the container in Azure Blob storage is enabled for anonymous access.  

https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&preserve-view=true&tabs=dedicated  

Now wea can create our file format:  

```sql
                CREATE EXTERNAL FILE FORMAT TextFileFormat WITH (
                        FORRMAT_TYPE = DELIMITEDTEXT,
                        FORMAT_OPTIONS(
                              FIELD_TERMINATOR=',',
                              FIRST_ROW = 2
                              )
                )
```

Now we get to specify details of columns with their types with the **location**, **Data Source** and the **file format** name we created already:  
**If the headers contains space we can encapsulate them between brackets. They should be the same as the columns we specify in the CREATE query**

```sql
              CREATE EXTERNAL TABLE { database_name.schema_name.table_name | schema_name.table_name | table_name }
                  ( <column_definition> [ ,...n ] )
                  WITH (
                      LOCATION = 'hdfs_folder_or_filepath',
                      DATA_SOURCE = external_data_source_name,
                      FILE_FORMAT = external_file_format_name
                      [ , <reject_options> [ ,...n ] ]
                  )
```
**The location is either a folder to read all files at once or the exact file so that the create table reads from it.**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7769bd45-d1ef-4964-b73a-d713c94f6731)  

##### External Tables for parquet files:

When dealing with parquet files we need to specify the compression type used so it decompresse it an render the data from the binary format:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/baded98c-42e6-4211-9e97-9840cc9f8dbf)  

```sql
--Create an external file format for PARQUET files.
CREATE EXTERNAL FILE FORMAT file_format_name
WITH (
         FORMAT_TYPE = PARQUET
     [ , DATA_COMPRESSION = {
        'org.apache.hadoop.io.compress.SnappyCodec'
      | 'org.apache.hadoop.io.compress.GzipCodec' }
    ]);
```

**Note that in parquet files, headers are not allowed to have spaces. Also check the columns types in the parquet files given in the first part of the file.**  

##### OPENROWSET:  

It is a powerful function that allows you to access files in Azure Storage. OPENROWSET function reads content of a remote data source (for example file) and returns the content as a set of rows. Within the serverless SQL pool resource, the OPENROWSET bulk rowset provider is accessed by calling the OPENROWSET function and specifying the BULK option.  
The OPENROWSET function can be referenced in the FROM clause of a query as if it were a table name OPENROWSET. It supports bulk operations through a built-in BULK provider that enables data from a file to be read and returned as a rowset.  

**Authentication :** By default, OPENROWSET uses the Azure Active Directory to access the files in an ADLS Gen2 for example. So if we have access as users to the ADLS we can use OPENROWSET without credentials.  

#### Dedicated SQL pool:  

the dedicated SQL pools are needed for creating a real data warehouse (very structured data) to serve reporting needs. Note that the pricing is per hour of data processed and it is much more expensive then the serverless one.In the analytical pools section in synapse workspace we can create our dedicated pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/c145d21c-5259-42e9-9220-887aedf25833)  

Once clicking on create a new SQL pool (the plus button), we can set the caracteristics of our pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/3beeec35-e83e-473e-820a-6572a486ee96)  

In opposition to serverless pools (where we can create only views and external tables), we can see that in dedicated pool we have a real database with tables views, external tables etc.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7064db39-3b2c-41db-9018-694e8c6a9b27)  

**Note that till now SQL dedicated pools don't support having multiple DBs in a single sql pool. Each pool have one single DB. (this is going to change and there will be a possibility to do crosse DB queries.**  

##### External tables in Dedicated pools:

For parquet files, Dedicated pools support natively the access to files. However for other formats it is not the case.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/26be217f-6d37-4f94-b3e6-c9af56f6aedd)  

For csv for example (and other formats), we need to create a hadoop external table to use it in order to access data.  

The authentication mode in SAS is not supported here so we should use access keys.  

Type| Dedicated Pools | Serverless Pools| 
| --- | --- | --- |
Storage authentication	| Storage Access Key(SAK), Microsoft Entra passthrough, Managed identity, custom application Microsoft Entra identity	| Shared Access Signature(SAS), Microsoft Entra passthrough, Managed identity, Custom application Microsoft Entra identity.|  

another main difference is that Folder partition elimination and File elimination (predicate pushdown) are not available in hadoop external tables.  

**Folder partition elimination**  
The native external tables in Synapse pools are able to ignore the files placed in the folders that are not relevant for the queries. If your files are stored in a folder hierarchy (for example - /year=2020/month=03/day=16) and the values for year, month, and day are exposed as the columns, the queries that contain filters like year=2020 will read the files only from the subfolders placed within the year=2020 folder. The files and folders placed in other folders (year=2021 or year=2022) will be ignored in this query. This elimination is known as partition elimination.  
The folder partition elimination is available in the native external tables that are synchronized from the Synapse Spark pools. If you have partitioned data set and you would like to leverage the partition elimination with the external tables that you create, use the partitioned views instead of the external tables.

**File elimination**  
Some data formats such as Parquet and Delta contain file statistics for each column (for example, min/max values for each column). The queries that filter data will not read the files where the required column values do not exist. The query will first explore min/max values for the columns used in the query predicate to find the files that do not contain the required data. These files will be ignored and eliminated from the query plan. This technique is also known as filter predicate pushdown and it can improve the performance of your queries. Filter pushdown is available in the serverless SQL pools on Parquet and Delta formats. To leverage filter pushdown for the string types, use the VARCHAR type with the Latin1_General_100_BIN2_UTF8 collation. For more information on collations, refer to Collation types supported for Synapse SQL.  

more in : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop  

The external data source for external tables (other than parquet) in dedicated pools should be changed as follows:  

```SQL
                  CREATE EXTERNAL DATA SOURCE AzureDataLakeStore
                  WITH
                    -- Please note the abfss endpoint when your account has secure transfer enabled
                    ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' , -- no need to add /blob name
                      CREDENTIAL = ADLS_credential ,
                      TYPE = HADOOP
                    ) ;
```

the lacation is now using **hdfs** protocol and **dfs endpoint** instead of blob to access the storage URL. Also note that we specify the name of the container before the **@**.  

Note that since we are forced to use hadoop external tables, the endpoint should be dfs. the difference between dfs and blob is the REST API used to access the storage:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/8dabc61c-8f99-49c8-914e-c9ea873e3ecc)  

**So hadoop external tables needs ADLS Gen2 (Hierarchical namespace activated) to connect to data.**  

|External Data Source	| endpoint |
| --- | --- |
|Azure Blob Storage	| .blob |
|ADLS Gen 2	| .dfs |  

Example of creating an external table in Dedicated SQL Pool:  

```SQL
                            CREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential
                            WITH
                              IDENTITY = 'datalake2000233',
                              SECRET = 'ZuTNLYAYPx1gmjZHE2+pIizxi4zwOHBYX395HDVQ/wM47yfhHBEBppzMKVegfROIi99SCwCWD04F+AStQjvUxw==';
                            
                            
                            CREATE EXTERNAL DATA SOURCE log_data
                            WITH (    LOCATION   = 'abfss://csv@datalake2000233.dfs.core.windows.net', -- before the @ we specify the blob container name
                                      CREDENTIAL = AzureStorageCredential,
                                      TYPE = HADOOP
                            )
                            
                            
                            CREATE EXTERNAL FILE FORMAT TextFileFormat WITH (  
                                  FORMAT_TYPE = DELIMITEDTEXT,  
                                FORMAT_OPTIONS (  
                                    FIELD_TERMINATOR = ',',
                                    FIRST_ROW = 2))
                            
                            
                            
                            CREATE EXTERNAL TABLE [logdata]
                            (
                                [Correlation id] [varchar](200) NULL,
                            	[Operation name] [varchar](200) NULL,
                            	[Status] [varchar](100) NULL,
                            	[Event category] [varchar](100) NULL,
                            	[Level] [varchar](100) NULL,
                            	[Time] [varchar] NULL, 
                            	[Subscription] [varchar](200) NULL,
                            	[Event initiated by] [varchar](1000) NULL,
                            	[Resource type] [varchar](1000) NULL,
                            	[Resource group] [varchar](1000) NULL,
                                [Resource] [varchar](2000) NULL
                            )
                            WITH (
                             LOCATION = '/Log.csv',
                                DATA_SOURCE = log_data,  
                                FILE_FORMAT = TextFileFormat
                            )

```

Note that if we use [datetime] for the directly for the [Time] column, it will throw an error since the hadoop driver does not make the conversion by default from files using external tables (it is only supported in the serverless SQL pool). We need to specify the date format in the file format properties:  

```SQL
                          CREATE EXTERNAL FILE FORMAT csvFile_ddMMyyyy_fr2
                          WITH (
                              FORMAT_TYPE = DELIMITEDTEXT,
                              FORMAT_OPTIONS (
                                FIELD_TERMINATOR = ',',
                                STRING_DELIMITER = '"',
                               DATE_FORMAT = 'dd/MM/yyyy',
                                FIRST_ROW = 2,
                                USE_TYPE_DEFAULT = TRUE,
                                ENCODING = 'UTF8' )
                          );
```

**To reduce cost, we need to pause the dedicated SQL pool when not used! we can also create budgets to be notified when a limit is reached.**  

##### Loading data into persistent tables using COPY statement:

Loading data into the persistent table is done using the **COPY INTO** query:  

```SQL
                            COPY INTO [ schema. ] table_name
                            [ (Column_list) ]
                            FROM '<external_location>' [ , ...n ]
                            WITH
                             (
                             [ FILE_TYPE = { 'CSV' | 'PARQUET' | 'ORC' } ]
                             [ , FILE_FORMAT = EXTERNAL FILE FORMAT OBJECT ]
                             [ , CREDENTIAL = (AZURE CREDENTIAL) ]
                             [ , ERRORFILE = ' [ http(s)://storageaccount/container ] /errorfile_directory [ / ] ] '
                             [ , ERRORFILE_CREDENTIAL = (AZURE CREDENTIAL) ]
                             [ , MAXERRORS = max_errors ]
                             [ , COMPRESSION = { 'Gzip' | 'DefaultCodec' | 'Snappy' } ]
                             [ , FIELDQUOTE = 'string_delimiter' ]
                             [ , FIELDTERMINATOR =  'field_terminator' ]
                             [ , ROWTERMINATOR = 'row_terminator' ]
                             [ , FIRSTROW = first_row ]
                             [ , DATEFORMAT = 'date_format' ]
                             [ , ENCODING = { 'UTF8' | 'UTF16' } ]
                             [ , IDENTITY_INSERT = { 'ON' | 'OFF' } ]
                             [ , AUTO_CREATE_TABLE = { 'ON' | 'OFF' } ]
                            )
```

Example :

```SQL
                          COPY INTO [pool_logdata] FROM 'https://datalake244434.blob.core.windows.net/csv/Log.csv'
                          WITH(
                              FILE_TYPE='CSV',
                              CREDENTIAL=(IDENTITY='Storage Account Key',SECRET='vDV2bSKSR44lbE6x05HtFz57DvlK3O2WNkb11te+H+GrBjeXCojnHjiTw3KdYBWXJRSAnOAZNdgB+AStAasz8w=='),
                              FIRSTROW=2)
```

**Loading data can be done also using Bulk Load wizard in synapse:**  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/ba7a41ec-4743-46fe-9658-8c9d50853d1f)  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/846f4c31-989c-4b9b-8230-c33355dc3ceb)  

##### DMV's for external ressources in synapse:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/80f75702-f956-4468-a0ee-506a627b94ed)  

##### Loading data using PolyBase:  

Loading data from external storages like ADLS Gen2, PolyBase is the fastest way.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/c8a3d2f9-68f6-4c43-bae8-d9a566c30f6f)  

PolyBase is a distributed compute engine inside SQL to improve performance when querying distributed data (Big Data).  

PolyBase works as follows:  

To COPY data using that, we just create extenal ressources (source, file format and external tables) from a blob storage and then copy into the persistent table from the corresponding external table.  

PolyBase with T-SQL works well when your data is in Azure Blob storage or Azure Data Lake Store. It gives you the most control over the loading process, but also requires you to define external data objects. The other methods define these objects behind the scenes as you map source tables to destination tables. To orchestrate T-SQL loads, you can use Azure Data Factory, SSIS, or Azure functions.  

If the data is not compatible with PolyBase, we can use bcp or the SQLBulkCopy API. BCP loads directly to dedicated SQL pool without going through Azure Blob storage, and is intended only for small loads. Note, the load performance of these options is slower than PolyBase.  

**The two options labeled “Polybase” and the “COPY command” are only applicable to Azure Synapse Analytics (formerly Azure SQL Data Warehouse). They are both fast methods of loading which involve staging data in Azure storage (if it’s not already in Azure Storage) and using a fast, highly parallel method of loading to each compute node from storage. Especially on large tables these options are preferred due to their scalability but they do come with some restrictions documented at the link above.**  

**In contrast, on Azure Synapse Analytics a bulk insert is a slower load method which loads data through the control node and is not as highly parallel or performant. It is an order of magnitude slower on large files. But it can be more forgiving in terms of data types and file formatting.**  

**On other Azure SQL databases, bulk insert is the preferred and fast method.**  

More regarding conditions to use Polybase and COPY statement:  

https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#use-copy-statement  

##### Load data using Synapse Pipelines:  

Pipelines (Same logic as in ADF)  are available in the integrate section in synapse workspace.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/52d79cd9-79a1-41d6-b8c3-53263062bb9a)  

We can create an easy standard pipeline that will copy data using the **COPY DATA TOOL**:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/74f80300-c842-4d80-ac80-9f98a31a2d0b)  

We set the source, the destination (new table or already existing table), and the settings. In the source we can sepcify filters on data to copy using SQL queries:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/75a0bb61-f67c-4337-b3eb-5672953fdf39)  

We can also see the mapping between the fields of Source and destination.  

In the settings we can set the copy method to use:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/7e7cad84-94b8-42a7-be19-1a3529968532)  

Once done, it will simply create a standard pipeline with an copy activity that we can monitor in the **Monitor** section :  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/098947e4-105e-4dc8-8c00-e0aae8dfacac)  

More in :

https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool?tabs=data-factory  

##### Creating the tables in the Datawarehouse:  

We can now create Fact and dimension tables in order to load data in it. **Note however, that in the synapse datawarehouse there no concept of constraints on keys like in transactional databases. we simply use joins to aggregate data later.**  
In many data warehouses, these constraints are not required, because data integrity is already guaranteed by the load jobs (e.g. key lookups of the surrogate keys in dimension tables). However, it is recommended to define the foreign key constraints anyway, it helps in documenting the data warehouse.  

#### Synapse Architecture and table types:

The architecture of synapse differese depending on the type of the pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e41a3e9b-9141-4c69-8603-4aa6c2e98310)  

In Dedicated SQL pool (formerly SQL DW), synapse uses a node-based architecture. Applications connect and issue T-SQL commands to a **Control node**. The Control node hosts the distributed query engine, which optimizes queries for parallel processing, and then passes operations to **Compute nodes** to do their work in parallel, once the job is done by the compute nodes, they send the answer back to the control node that communicates with the Application/User. **Note that each node has a SQL endpoint.**  
The Compute nodes store all user data in Azure Storage and run the parallel queries. The Data Movement Service (DMS) is a system-level internal service that moves data across the nodes as necessary to run queries in parallel and return accurate results.  
With decoupled storage and compute, when using a dedicated SQL pool (formerly SQL DW) one can:  
- Independently size compute power irrespective of your storage needs.
- Grow or shrink compute power, within a dedicated SQL pool (formerly SQL DW), without moving data.
- Pause compute capacity while leaving data intact, so you only pay for storage.
- Resume compute capacity during operational hours.

In this architecture we have:  

- Compute and storage layers are independent
- The compute storage can scale independently
- We can pause the computing layer (but we still get charged for the storage)
- The dedicated pool compute engine is charged in DWU (data warehouse unit, **A combination of compute, memory and IO**)

In the storage part, the data is shared or split into distributions (**by default 60 distribution**) to optimize the performance. We can check the ditributions of our tables using the following query:  

```SQL
                          DBCC PDW_SHOWSPACEUSED('[dbo].[FactSales]')
```

There are 3 types of tables created in Synapse:
- Hash-distributed tables
- Round-Robin distributed tables (By default if we don't specify the type in the creation)
- Replicated tables

The Hash-distributed tables use a hash function to decide which data to go in which distribution **based on a column** (Rows with the same ID column, if we use it in the hashing function, for exapmle are stored in the same distribution). The power of hashed tables is that they are faster when we filter data and when we make joins based in the hashing column since synapse will go directly to the corresponding distribution without searching the whole data in all distribution.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/5997dc66-7ce2-497a-ac9c-d6bd94fab1b4)  

The Round **Robin however**, distributes data evenly across the table but without any further optimization. A distribution is first chosen at random and then buffers of rows are assigned to distributions sequentially **(if we have 60 distribution first row goes to first distribution second row to secon and so on then 61st row to 61st distribution and so on)**. It is quick to load data into a round-robin table, but query performance can often be better with hash distributed tables. **Joins on round-robin tables require reshuffling data, which takes additional time.**  
**Round-Robin distribution is best suited for staging purposes since it is fast in terms of loading data (no hash function step needed to load data)**

Under the monitoring section, we can see the history of queries and analyze the details of their execution to compare the round robin tables against the hash tables : 

**Round Robin table**:  

![image](https://github.com/user-attachments/assets/5a444aa6-6ac4-49f7-b45d-847f7071945e)

**Hash table**:  

![image](https://github.com/user-attachments/assets/197e2044-c2ea-4a7a-a519-d012faf7f9a1)  

**We can see that in terms of reading data, the hash tables are more performant since non shuffle is needed**  

The replicated tables in the other hand, provides the fastest query performance for small tables. A table that is replicated caches a full copy of the table on each compute node. Consequently, replicating a table removes the need to transfer data among compute nodes before a join or aggregation. Replicated tables are best utilized with small tables. Extra storage is required and there is additional overhead that is incurred when writing data, which make large tables impractical.  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/6f31c345-c171-4726-9817-085234c66358)  

**For fact tables we can use Hashed Tables, and for dimension tables (since they  are smaller) we can use Replicated tables for fast performance.**  

For each distribution, we don't have necesserly a compute node. It depends on the configuration **(Data warehouse size)**  we choose for our dedicated SQL pool:  

| Data warehouse units	| # of compute nodes	| # of distributions per node |
| --- | --- | --- |
| DW100c	| 1	| 60 |
| DW200c	| 1	| 60 |
|DW300c	| 1	| 60 |
|DW400c	| 1	| 60 |
|DW500c	| 1	| 60 |
|DW1000c	| 2 |	30 |
|DW1500c	| 3	| 20 |
|DW2000c	| 4	| 15 |
|DW2500c	| 5	| 12 |
|DW3000c	| 6	| 10 |
|DW5000c	| 10 |	6 |
|DW6000c	| 12 |  5 |
|DW7500c	| 15 |	4 |
|DW10000c |	20 |	3 |
|DW15000c |	30 |	2 |
|DW30000c	| 60 | 1 |

**Note that Serverless SQL does not support data distribution.**  

More in : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview  

We can use the following query to check for the tables types:  

```SQL
                  SELECT
                      OBJECT_NAME (object_id) AS table_name,
                  FROM sys.pdw_table_distribution_properties;
```

In the monitor section, we can see the history of our queries against our dedicated sql pool:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/e40b990a-7555-4a20-850e-3163ba98983e)  

When we click on a request ID we can have details regarding the operations done behind the scene:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/a50e8450-516a-41aa-9376-e87665cc362f)  

#### Create Hash-distribution tables:  

We can create hash-distribution tables using the following query:  

```SQL
                        CREATE TABLE [dbo].[FactSales](
                        	[ProductID] [int] NOT NULL,
                        	[SalesOrderID] [int] NOT NULL,
                        	[CustomerID] [int] NOT NULL,
                        	[OrderQty] [smallint] NOT NULL,
                        	[UnitPrice] [money] NOT NULL,
                        	[OrderDate] [datetime] NULL,
                        	[TaxAmt] [money] NULL
                        )
                        WITH  
                        (   
                            DISTRIBUTION = HASH (CustomerID)
                        )
                        
                        -- To see the distribution on the table
                        DBCC PDW_SHOWSPACEUSED('[dbo].[FactSales]')

```

Once created and populated, we can query (when the query contains the column used for creating the hash distributions, the query is faster) the data and we will see the details regarding operations done behind the scene:  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/52e64088-d4d1-40c8-8bd1-49baed1059ee)  

In opposition to Round Robin tables, Hash-distribution are much faster since the only compute (No data shuffling ...).  

![image](https://github.com/ZACKHADD/Data_Codes_Steps/assets/59281379/1b815c71-e6a0-4d13-b42a-886a27532b2a)  

The choice of the column to use is so important and it must meet some creteria :  
- **it is used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses.**
- Has many unique values
- Does not have NULLs, or has only a few NULLs
- Is not a date column
- **Is not used in WHERE clauses**

More details on :  

https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute  

To check the distribution of our table accross the distributions we use the following command:  

```SQL
      -- Find data skew for a distributed table
      DBCC PDW_SHOWSPACEUSED('dbo.FactInternetSales');
```

### Surrogate key:

As the normal databases we can create surrogete keys to be used in our slowly changing dimension tables using the "IDENTITY(1,1)" command.  

```SQL

              CREATE TABLE dbo.T1
              (    C1 INT IDENTITY(1,1) NOT NULL
              ,    C2 INT NULL
              )
              WITH
              (   DISTRIBUTION = HASH(C2)
              ,   CLUSTERED COLUMNSTORE INDEX
              )
              ;
```

We can also use a command to insert predefined values (like négative values for unknown members) in our surrogate key column.  

https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity  

### Indexes :

By default, dedicated SQL pool creates a clustered columnstore index when no index options are specified on a table. Clustered columnstore tables offer both the highest level of data compression and the best overall query performance.  

When using small tables (less than 60 million rows) or when loading to staging area we need to consider using temporary tables **HEAP** that makes the load faster, then create indexes for faster query performance.  

```SQL

                      CREATE TABLE myTable
                        (  
                          id int NOT NULL,  
                          lastName varchar(20),  
                          zipCode varchar(6)  
                        )  
                      WITH ( CLUSTERED COLUMNSTORE INDEX );

```
After a load completes, some of the data rows might not be compressed into the columnstore. There are different reasons why this can happen.  
To optimize query performance and columnstore compression after a load, rebuild the table to force the columnstore index to compress all the rows.  

```SQL
                      SELECT GETDATE();
                      GO
                      
                      ALTER INDEX ALL ON [cso].[DimProduct]               REBUILD;
                      ALTER INDEX ALL ON [cso].[FactOnlineSales]          REBUILD;
```

**Clustered Columnstore indexes are the best choice in terms of performance**  
https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index  

### Which load method to use:

The Bulk insert method is simple but not faster as it moves all the data throught the control node first.  
The COPY INTO is faster than the bulk insert and simpler than the polybase method since you don't need to define external data source, data files and data tables. However the PolyBase method is much faster as it runs the create table in the compute nodes in parallel.  

### Partitions :

Anytime you design for Synapse Analytics, you must remember the law of 60. Data is being partitioned over 60 nodes already. Anytime your partition strategy covers this law optimally, then Synapse Analytics will shine. As a guide and example, when creating partitions and clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstores, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, Synapse Analytic pools already divides each table into 60 distributed databases. So any partitioning adding to a table is in addition to the distribution some partitions created behind the scenes to the 60 computer nodes. Using this example, if the sales fact table contained 36 monthly partitions, and given that a dedicated SQL pool has 60 nodes per partition, then the sales fact table should contain 60 million rows per month, or 2.1 billion rows when all the months are populated to say that it is a good partition strategy to partition by month. If a table contains fewer than the recommended minimum rows per partition, consider using fewer partitions to increase the number of rows per partition. The fact that tables are already divided into 60 internal partitions is called table distribution, and comparing it correctly alongside the table partitions will help drastically your application.  

![image](https://github.com/user-attachments/assets/d0d76db7-8cbc-406d-b8de-f1ca29d28c0b)  

**After distributing data across distributions, partitioning further divides data within each distribution based on a specified column (partition key).**    
The partitioning key : 
- Should be frequently used in queries for filtering (e.g., date column for time-series data).
- Should have high cardinality to distribute data evenly across partitions.
- Ideally, avoid columns with high data skew or a large number of NULL values.

**Partitioning data have a big impact on data maintenance such as loading new data or deleting/Archiving old data (The deletion will be done one shot on the partition instead of row per row which is heavy on the transactional logs)**  

![image](https://github.com/user-attachments/assets/585c6fd1-6655-471b-9229-e220858a5790)  

The stage table is a temporary table (HEAP) which makes the load even faster.  

**Avoid overpartitioning as it may severly impacts the performance when the number of rows per partition is low (less than 1 million per partition par distribution)**  

Typically we use dates to partition data using right or left range:

![image](https://github.com/user-attachments/assets/7e4b367e-2c8e-44f8-84da-b2ed308ab83c)  

```SQL

              CREATE TABLE [dbo].[FactInternetSales]
                (
                    [ProductKey]            int          NOT NULL
                ,   [OrderDateKey]          int          NOT NULL
                ,   [CustomerKey]           int          NOT NULL
                ,   [PromotionKey]          int          NOT NULL
                ,   [SalesOrderNumber]      nvarchar(20) NOT NULL
                ,   [OrderQuantity]         smallint     NOT NULL
                ,   [UnitPrice]             money        NOT NULL
                ,   [SalesAmount]           money        NOT NULL
                )
                WITH
                (   CLUSTERED COLUMNSTORE INDEX
                ,   DISTRIBUTION = HASH([ProductKey])
                ,   PARTITION   (   [OrderDateKey] RANGE RIGHT FOR VALUES
                                    (20000101,20010101,20020101
                                    ,20030101,20040101,20050101
                                    )
                                )
                );

```

https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition  

**Switch Out partitions (Delete or archive):**  

```SQL

    --HACK to create a table with the same definition but without data
                CREATE TABLE [cso].[FactOnlineSales_out]
                  WITH 
                  (   DISTRIBUTION=HASH ([ProductKey])
                  ,   CLUSTERED COLUMNSTORE INDEX
                  ,   PARTITION ([DateKey] 
                      RANGE RIGHT FOR VALUES 	
                  	(
                  		'2007-01-01 00:00:00.000'
                      ))
                  )
                  AS 
                  SELECT * FROM [cso].[FactOnlineSales_Partitioned] WHERE 1=2;
-- the false statement gives an empty table and the CTAS commande gives the same definition as the other table.

-- Now we move data from the original partition to the new table we created (staging table)

              ALTER TABLE [cso].[FactOnlineSales_Partitioned] 
              SWITCH PARTITION 2 
              TO [cso].[FactOnlineSales_out] PARTITION 2;

-- Then we can either drop or archive the new table containing the partition

-- To check number of rows per partition

                SELECT  pnp.partition_number, sum(nps.[row_count]) AS Row_Count
                FROM
                   sys.tables t
                INNER JOIN sys.indexes i
                    ON  t.[object_id] = i.[object_id]
                    AND i.[index_id] <= 1 /* HEAP = 0, CLUSTERED or CLUSTERED_COLUMNSTORE =1 */
                INNER JOIN sys.pdw_table_mappings tm
                    ON t.[object_id] = tm.[object_id]
                INNER JOIN sys.pdw_nodes_tables nt
                    ON tm.[physical_name] = nt.[name]
                INNER JOIN sys.pdw_nodes_partitions pnp 
                    ON nt.[object_id]=pnp.[object_id] 
                    AND nt.[pdw_node_id]=pnp.[pdw_node_id] 
                    AND nt.[distribution_id] = pnp.[distribution_id]
                INNER JOIN sys.dm_pdw_nodes_db_partition_stats nps
                    ON nt.[object_id] = nps.[object_id]
                    AND nt.[pdw_node_id] = nps.[pdw_node_id]
                    AND nt.[distribution_id] = nps.[distribution_id]
                    AND pnp.[partition_id]=nps.[partition_id]
                WHERE t.name='FactOnlineSales_out'
                GROUP BY pnp.partition_number;

```
**Data Partitioning best practices :**  

https://github.com/AasTrailblazers/AzureSynapse/tree/main/SQL%20pool/Table%20Partition%20and%20Best%20Practices  

### Statistics:

It's best to create single-column statistics immediately after a load. If you know certain columns aren't going to be in query predicates, you can skip creating statistics on those columns. If you create single-column statistics on every column, it might take a long time to rebuild all the statistics.  

```SQL
      CREATE STATISTICS [stat_cso_DimProduct_AvailableForSaleDate] ON [cso].[DimProduct]([AvailableForSaleDate]);
```
Remember to update statistics after creattion of every table for optimal performance:  

```SQL
              UPDATE statistics [cso].[FactOnlineSales_Partitioned]
```

### Create a user for loading data:

The server admin account is meant to perform management operations, and is not suited for running queries on user data. Loading data is a memory-intensive operation. Memory maximums are defined according to the Generation of SQL pool you're using, data warehouse units, and **resource class**.  

## Data Warehouse DEMO :

We will create in this demo a dedicated SQL pool using all the above techniques to load data into the warehouse.  
The DW schema (star) is the following :  

![image](https://github.com/user-attachments/assets/196e7433-a063-4ffb-94df-4a4a85aa5307)  

More details on :  
https://dataedo.com/samples/html/Data_warehouse/doc/AdventureWorksDW_4/modules/Internet_Sales_101/module.html  


